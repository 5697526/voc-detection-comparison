{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c6b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm\n",
    "\n",
    "# 日志文件路径（替换为你的实际路径）\n",
    "log_file = r\"D:\\FDU\\学习\\24-25大三下\\计算机视觉\\pj2\\mmdetection\\work_dirs\\sparse-rcnn-cuda\\20250518_101231\\20250518_101231.log\"\n",
    "# TensorBoard日志输出目录\n",
    "tb_log_dir = r\"D:\\FDU\\codefield\\work\\sparse1\"\n",
    "os.makedirs(tb_log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=tb_log_dir)\n",
    "\n",
    "# 正则表达式\n",
    "train_re = re.compile(\n",
    "    r'Epoch\\(train\\)\\s+\\[(\\d+)\\]\\[(\\d+)/(\\d+)\\].*?lr:\\s+([\\d.e-]+).*?loss:\\s+([\\d.e-]+)')\n",
    "val_map_re = re.compile(r'mAP\\s+.*?\\|\\s+.*?\\|\\s+.*?\\|\\s+.*?\\|\\s+([\\d.e-]+)')\n",
    "stage_loss_pattern = re.compile(r's(\\d+)\\.(loss_[a-z]+):\\s+([\\d.e-]+)')\n",
    "pos_acc_pattern = re.compile(r's(\\d+)\\.pos_acc:\\s+([\\d.e-]+)')\n",
    "\n",
    "# 初始化变量\n",
    "current_epoch = 0\n",
    "train_step = 0\n",
    "\n",
    "# 记录被过滤的异常值数量\n",
    "filtered_count = 0\n",
    "\n",
    "# 获取日志行数用于进度显示\n",
    "try:\n",
    "    with open(log_file, 'r', encoding='utf-8') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "except Exception as e:\n",
    "    total_lines = None\n",
    "    print(f\"无法获取日志行数: {e}\")\n",
    "\n",
    "print(\"开始解析日志文件...\")\n",
    "with open(log_file, 'r', encoding='utf-8') as f:\n",
    "    lines = tqdm.tqdm(f, total=total_lines, desc=\"解析日志\") if total_lines else f\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # 解析训练阶段日志\n",
    "        train_match = train_re.search(line)\n",
    "        if train_match:\n",
    "            epoch = int(train_match.group(1))\n",
    "            iter_in_epoch = int(train_match.group(2))\n",
    "            total_iter = int(train_match.group(3))\n",
    "            lr = float(train_match.group(4))\n",
    "            total_loss = float(train_match.group(5))\n",
    "\n",
    "            # 更新当前epoch和step\n",
    "            if epoch > current_epoch:\n",
    "                current_epoch = epoch\n",
    "                print(f\"处理第 {current_epoch} 个epoch\")\n",
    "\n",
    "            # 计算全局step\n",
    "            train_step = (current_epoch - 1) * total_iter + iter_in_epoch\n",
    "\n",
    "            # 写入基本指标\n",
    "            writer.add_scalar('Train/LR', lr, train_step)\n",
    "            writer.add_scalar('Train/Loss/Total', total_loss, train_step)\n",
    "\n",
    "            # 提取所有阶段的损失值\n",
    "            for match in stage_loss_pattern.finditer(line):\n",
    "                stage = int(match.group(1))\n",
    "                loss_type = match.group(2)\n",
    "                loss_value = float(match.group(3))\n",
    "\n",
    "                # 只记录loss_cls <= 1的值，其他损失正常记录\n",
    "                if loss_type == 'loss_cls' and loss_value > 1:\n",
    "                    filtered_count += 1\n",
    "                    continue  # 跳过大于1的loss_cls值\n",
    "\n",
    "                writer.add_scalar(\n",
    "                    f'Train/Loss/Stage{stage}_{loss_type}', loss_value, train_step)\n",
    "\n",
    "            # 提取所有阶段的pos_acc\n",
    "            for match in pos_acc_pattern.finditer(line):\n",
    "                stage = int(match.group(1))\n",
    "                pos_acc = float(match.group(2))\n",
    "                writer.add_scalar(\n",
    "                    f'Train/PosAcc/Stage{stage}', pos_acc, train_step)\n",
    "\n",
    "        # 解析验证阶段mAP\n",
    "        val_map_match = val_map_re.search(line)\n",
    "        if val_map_match:\n",
    "            val_mAP = float(val_map_match.group(1))\n",
    "            writer.add_scalar('Val/mAP', val_mAP, current_epoch)\n",
    "\n",
    "writer.close()\n",
    "print(f\"TensorBoard日志已生成，路径：{tb_log_dir}\")\n",
    "print(f\"共过滤 {filtered_count} 个超过1的loss_cls值\")\n",
    "print(\"请使用命令 tensorboard --logdir \\\"{tb_log_dir}\\\" 启动TensorBoard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm\n",
    "\n",
    "# ---------------------- 配置参数 ----------------------\n",
    "# 日志文件路径（替换为你的实际路径）\n",
    "log_file = r\"D:\\FDU\\学习\\24-25大三下\\计算机视觉\\pj2\\mmdetection\\work_dirs\\mask-rcnn-cuda\\20250516_095727\\20250516_095727.log\"\n",
    "# TensorBoard日志输出目录\n",
    "tb_log_dir = r\"D:\\FDU\\codefield\\work\\mask-rcnn-tb-logs\"  # 可自定义路径\n",
    "os.makedirs(tb_log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=tb_log_dir)\n",
    "\n",
    "# ---------------------- 正则表达式定义 ----------------------\n",
    "# 训练阶段指标（Epoch(train)行）\n",
    "train_re = re.compile(\n",
    "    r'Epoch\\(train\\)\\s+\\[(\\d+)\\]\\[(\\d+)/(\\d+)\\].*?lr:\\s+([\\d.e-]+).*?'\n",
    "    r'loss:\\s+([\\d.e-]+).*?loss_rpn_cls:\\s+([\\d.e-]+).*?loss_rpn_bbox:\\s+([\\d.e-]+).*?'\n",
    "    r'loss_cls:\\s+([\\d.e-]+).*?acc:\\s+([\\d.e-]+).*?loss_bbox:\\s+([\\d.e-]+)'\n",
    ")\n",
    "\n",
    "# 验证阶段mAP（表格中的mAP值）\n",
    "val_map_re = re.compile(\n",
    "    r'mAP\\s+\\|\\s+.*?\\|\\s+.*?\\|\\s+([\\d.e-]+)')  # 匹配最后一列的mAP值\n",
    "\n",
    "# ---------------------- 初始化变量 ----------------------\n",
    "current_epoch = 0\n",
    "train_step = 0  # 训练步数（以迭代次数为横坐标）\n",
    "val_epoch = 0   # 验证阶段的epoch数\n",
    "\n",
    "# ---------------------- 解析日志文件 ----------------------\n",
    "print(\"开始解析日志文件...\")\n",
    "with open(log_file, 'r', encoding='utf-8') as f:\n",
    "    # 使用tqdm显示进度条（可选）\n",
    "    # Windows可能不支持tqdm，可注释掉\n",
    "    lines = tqdm.tqdm(f, desc=\"解析日志\", unit=\"行\") if os.name != 'nt' else f\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # ---------------------- 解析训练阶段 ----------------------\n",
    "        train_match = train_re.search(line)\n",
    "        if train_match:\n",
    "            # 提取训练指标\n",
    "            epoch = int(train_match.group(1))\n",
    "            iter_in_epoch = int(train_match.group(2))\n",
    "            total_iter = int(train_match.group(3))\n",
    "            lr = float(train_match.group(4))\n",
    "            total_loss = float(train_match.group(5))\n",
    "            loss_rpn_cls = float(train_match.group(6))\n",
    "            loss_rpn_bbox = float(train_match.group(7))\n",
    "            loss_cls = float(train_match.group(8))\n",
    "            acc = float(train_match.group(9))\n",
    "            loss_bbox = float(train_match.group(10))\n",
    "\n",
    "            # 更新当前epoch和step（step为全局迭代次数）\n",
    "            if epoch > current_epoch:\n",
    "                current_epoch = epoch\n",
    "            train_step = (current_epoch - 1) * total_iter + iter_in_epoch\n",
    "\n",
    "            # 写入TensorBoard：训练阶段指标\n",
    "            writer.add_scalar('Train/Loss/Total', total_loss, train_step)\n",
    "            writer.add_scalar('Train/Loss/RPN_Cls', loss_rpn_cls, train_step)\n",
    "            writer.add_scalar('Train/Loss/RPN_Bbox', loss_rpn_bbox, train_step)\n",
    "            writer.add_scalar('Train/Loss/Cls', loss_cls, train_step)\n",
    "            writer.add_scalar('Train/Accuracy', acc, train_step)\n",
    "            writer.add_scalar('Train/Loss/Bbox', loss_bbox, train_step)\n",
    "            writer.add_scalar('Train/LR', lr, train_step)  # 学习率曲线\n",
    "\n",
    "        # ---------------------- 解析验证阶段 ----------------------\n",
    "        if \"mAP\" in line and \"class\" not in line:  # 过滤表头，只匹配mAP数值行\n",
    "            val_map_match = val_map_re.search(line)\n",
    "            if val_map_match:\n",
    "                val_mAP = float(val_map_match.group(1))\n",
    "                val_epoch = current_epoch  # 验证结果对应当前训练的epoch\n",
    "                writer.add_scalar('Val/mAP', val_mAP, val_epoch)\n",
    "\n",
    "writer.close()\n",
    "print(f\"TensorBoard日志已生成，路径：{tb_log_dir}\")\n",
    "print(\"请在终端运行：tensorboard --logdir \\\"{tb_log_dir}\\\" --port 6006\")\n",
    "print(\"然后在浏览器访问 http://localhost:6006 查看可视化结果\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f9261f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\mmengine\\optim\\optimizer\\zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import \\\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\mmengine\\config\\config.py\", line 109, in __getattr__\n",
      "    value = super().__getattr__(name)\n",
      "  File \"d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\addict\\addict.py\", line 67, in __getattr__\n",
      "    return self.__getitem__(item)\n",
      "  File \"d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\mmengine\\config\\config.py\", line 138, in __getitem__\n",
      "    return self.build_lazy(super().__getitem__(key))\n",
      "  File \"d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\mmengine\\config\\config.py\", line 105, in __missing__\n",
      "    raise KeyError(name)\n",
      "KeyError: 'default_hooks'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"tools/test.py\", line 149, in <module>\n",
      "    main()\n",
      "  File \"tools/test.py\", line 91, in main\n",
      "    cfg = trigger_visualization_hook(cfg, args)\n",
      "  File \"d:\\fdu\\学习\\24-25大三下\\计算机视觉\\pj2\\mmdetection\\mmdet\\engine\\hooks\\utils.py\", line 3, in trigger_visualization_hook\n",
      "    default_hooks = cfg.default_hooks\n",
      "  File \"d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\mmengine\\config\\config.py\", line 1500, in __getattr__\n",
      "    return getattr(self._cfg_dict, name)\n",
      "  File \"d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\mmengine\\config\\config.py\", line 113, in __getattr__\n",
      "    raise AttributeError(f\"'{self.__class__.__name__}' object has no \"\n",
      "AttributeError: 'ConfigDict' object has no attribute 'default_hooks'\n"
     ]
    }
   ],
   "source": [
    "!python tools/test.py configs/_base_/datasets/custom_dataset.py work_dirs/mask-rcnn-cuda/epoch_6.pth --show-dir work_dirs/mask-rcnn-cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3896d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mmcv\n",
    "import matplotlib.pyplot as plt\n",
    "from mmdet.apis import init_detector, inference_detector, show_result_pyplot\n",
    "\n",
    "# 设置中文字体显示\n",
    "plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\"]\n",
    "\n",
    "# 定义配置文件和模型权重路径\n",
    "config_file_mask = 'configs/mask_rcnn/mask_rcnn_r50_fpn_1x_voc0712.py'\n",
    "checkpoint_file_mask = 'checkpoints/mask_rcnn_r50_fpn_1x_voc0712_20200624_231812-95b326a5.pth'\n",
    "\n",
    "config_file_sparse = 'configs/sparse_rcnn/sparse_rcnn_r50_fpn_1x_voc0712.py'\n",
    "checkpoint_file_sparse = 'checkpoints/sparse_rcnn_r50_fpn_1x_voc0712_20200624_231812-95b326a5.pth'\n",
    "\n",
    "# 创建模型\n",
    "model_mask = init_detector(\n",
    "    config_file_mask, checkpoint_file_mask, device='cuda:0')\n",
    "model_sparse = init_detector(\n",
    "    config_file_sparse, checkpoint_file_sparse, device='cuda:0')\n",
    "\n",
    "# 图像文件夹路径和输出路径\n",
    "image_folder = 'data'\n",
    "output_folder = 'results'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 获取所有图像文件\n",
    "image_files = [f for f in os.listdir(\n",
    "    image_folder) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# 对每张图像进行推理和可视化\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(image_folder, image_file)\n",
    "    img = mmcv.imread(image_path)\n",
    "\n",
    "    # 模型推理\n",
    "    result_mask = inference_detector(model_mask, img)\n",
    "    result_sparse = inference_detector(model_sparse, img)\n",
    "\n",
    "    # 可视化并保存结果 - Mask R-CNN\n",
    "    out_file_mask = os.path.join(output_folder, f'mask_{image_file}')\n",
    "    show_result_pyplot(\n",
    "        model_mask,\n",
    "        img,\n",
    "        result_mask,\n",
    "        score_thr=0.5,\n",
    "        title=f'Mask R-CNN - {image_file}',\n",
    "        out_file=out_file_mask\n",
    "    )\n",
    "\n",
    "    # 可视化并保存结果 - Sparse R-CNN\n",
    "    out_file_sparse = os.path.join(output_folder, f'sparse_{image_file}')\n",
    "    show_result_pyplot(\n",
    "        model_sparse,\n",
    "        img,\n",
    "        result_sparse,\n",
    "        score_thr=0.5,\n",
    "        title=f'Sparse R-CNN - {image_file}',\n",
    "        out_file=out_file_sparse\n",
    "    )\n",
    "\n",
    "    print(f\"已处理图像: {image_file}\")\n",
    "    print(f\"Mask R-CNN结果已保存至: {out_file_mask}\")\n",
    "    print(f\"Sparse R-CNN结果已保存至: {out_file_sparse}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"所有图像均已处理完毕!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9825a09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/mask-rcnn-cuda/epoch_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\mmengine\\optim\\optimizer\\zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import \\\n",
      "d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\mmengine\\runner\\checkpoint.py:347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, map_location=map_location)\n",
      "Traceback (most recent call last):\n",
      "  File \"demo/image_demo.py\", line 192, in <module>\n",
      "    main()\n",
      "  File \"demo/image_demo.py\", line 179, in main\n",
      "    inferencer = DetInferencer(**init_args)\n",
      "  File \"d:\\fdu\\学习\\24-25大三下\\计算机视觉\\pj2\\mmdetection\\mmdet\\apis\\det_inferencer.py\", line 99, in __init__\n",
      "    super().__init__(\n",
      "  File \"d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\mmengine\\infer\\infer.py\", line 180, in __init__\n",
      "    self.model = self._init_model(cfg, weights, device)  # type: ignore\n",
      "  File \"d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\mmengine\\infer\\infer.py\", line 486, in _init_model\n",
      "    model.to(device)\n",
      "  File \"d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\mmengine\\model\\base_model\\base_model.py\", line 208, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "  File \"d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "  File \"d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 854, in _apply\n",
      "    self._buffers[key] = fn(buf)\n",
      "  File \"d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "  File \"d:\\Download\\Anaconda3\\envs\\mmdet\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 305, in _lazy_init\n",
      "    raise AssertionError(\"Torch not compiled with CUDA enabled\")\n",
      "AssertionError: Torch not compiled with CUDA enabled\n"
     ]
    }
   ],
   "source": [
    "!python demo/image_demo.py data/image1.jpg configs/mask-rcnn_voc.py --weights work_dirs/mask-rcnn-cuda/epoch_6.pth --show "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
